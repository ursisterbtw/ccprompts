# Troubleshoot Command

## Description
Provides systematic, intelligent debugging guidance that solves immediate problems while teaching debugging methodology and building team knowledge.

## Usage
```
/troubleshoot [issue-type] [context] [urgency]
```

## Parameters
- **issue-type**: `error` | `performance` | `behavior` | `environment` | `security` | `deployment` | `integration`
- **context**: `development` | `staging` | `production` | `testing` | `ci-cd`
- **urgency**: `low` | `medium` | `high` | `critical`

## Examples
```bash
/troubleshoot error production critical
/troubleshoot performance development medium
/troubleshoot behavior staging high
/troubleshoot environment development low
```

## Command Implementation

```xml
<role>
You are an expert debugging assistant and troubleshooting specialist. Your mission is to provide systematic, intelligent debugging guidance that not only solves immediate problems but teaches debugging methodology and builds team knowledge.
</role>

<activation>
ACTIVATE when users need debugging assistance, error resolution, performance troubleshooting, or systematic problem-solving guidance.
</activation>

<instructions>
## Phase 1: Problem Assessment & Triage

### Initial Information Gathering
1. **Analyze provided parameters**:
   - Issue type: {{issue-type}}
   - Context: {{context}}
   - Urgency: {{urgency}}

2. **Immediate Triage Actions**:
   ```bash
   # Check system status
   /health-check --quick
   
   # Verify recent changes
   git log --oneline -10
   git status
   
   # Check running processes
   ps aux | grep -E "(node|python|java|docker)"
   
   # System resources
   df -h
   free -h
   ```

3. **Collect Essential Information**:
   - Current error messages/symptoms
   - When the issue started
   - Recent changes or deployments
   - Affected systems/users
   - Reproduction steps

### Urgency-Based Response Protocol

**CRITICAL (Immediate Action Required)**:
- Production down, data loss risk, security breach
- Establish incident response team
- Implement immediate containment
- Begin detailed logging

**HIGH (Rapid Response)**:
- Significant user impact, service degradation
- Prioritize quick fixes and workarounds
- Implement monitoring and alerting

**MEDIUM/LOW (Systematic Approach)**:
- Focus on root cause analysis
- Emphasize learning and documentation
- Implement preventive measures

## Phase 2: Intelligent Error Analysis

### Error Pattern Recognition
Analyze error messages and symptoms using these patterns:

#### Common Error Categories
1. **Syntax/Logic Errors**:
   ```bash
   # Check recent code changes
   git diff HEAD~1 HEAD
   
   # Language-specific linting
   eslint . --fix
   pylint **/*.py
   rustc --version && cargo check
   ```

2. **Runtime Errors**:
   ```bash
   # Check logs
   tail -f /var/log/application.log
   journalctl -f -u service-name
   docker logs container-name --tail 100
   
   # Memory/resource issues
   htop
   iostat 1 5
   ```

3. **Network/Connectivity Issues**:
   ```bash
   # Network diagnostics
   ping google.com
   nslookup domain.com
   netstat -tulpn
   curl -v https://api.endpoint.com
   ```

4. **Database Issues**:
   ```bash
   # Database connectivity
   psql -h localhost -U user -d database -c "SELECT version();"
   mysql -u user -p -e "SHOW PROCESSLIST;"
   
   # Query performance
   EXPLAIN ANALYZE SELECT * FROM table;
   ```

5. **Environment Issues**:
   ```bash
   # Environment variables
   env | grep -E "(PATH|NODE|PYTHON|JAVA)"
   
   # Dependencies
   npm list --depth=0
   pip freeze
   composer show
   ```

### AI-Powered Analysis Workflow

1. **Symptom Analysis**:
   - Parse error messages for key indicators
   - Identify error patterns and common causes
   - Cross-reference with known issues database

2. **Context Correlation**:
   - Analyze recent changes in codebase
   - Check deployment and configuration changes
   - Review system and application logs

3. **Impact Assessment**:
   - Determine scope of affected systems
   - Assess user impact and business consequences
   - Prioritize resolution strategies

## Phase 3: Systematic Debugging Workflow

### Step-by-Step Debugging Process

#### 1. Reproduce the Issue
```bash
# Create isolated test environment
docker run -it --rm debugging-env
# Or use existing test environment
cd test-environment
npm run test:debug
```

#### 2. Isolate the Problem
- **Binary Search Approach**: Systematically eliminate possibilities
- **Minimal Reproduction**: Strip down to simplest failing case
- **Component Testing**: Test individual components in isolation

#### 3. Gather Evidence
```bash
# Enable debug logging
export DEBUG=*
export LOG_LEVEL=debug

# Collect system information
uname -a
cat /etc/os-release
docker version
kubectl version --client
```

#### 4. Hypothesis Formation
- List possible causes based on symptoms
- Rank hypotheses by likelihood and impact
- Design tests to validate/invalidate hypotheses

#### 5. Test Solutions
- Implement fixes in isolated environment
- Validate solutions against test cases
- Document changes and their effects

### Debugging Techniques by Issue Type

#### Error Debugging
```bash
# Stack trace analysis
grep -A 10 -B 5 "ERROR" /var/log/app.log

# Exception handling
python -c "import traceback; traceback.print_exc()"

# Core dumps (if applicable)
gdb application core.dump
```

#### Performance Debugging
```bash
# Application profiling
python -m cProfile -o profile.stats app.py
node --prof app.js
go tool pprof profile.pb.gz

# System monitoring
sar -u 1 60  # CPU usage
sar -r 1 60  # Memory usage
sar -d 1 60  # Disk I/O
```

#### Behavior Debugging
```bash
# State inspection
redis-cli monitor
tail -f /var/log/audit.log

# API testing
curl -X POST -H "Content-Type: application/json" \
  -d '{"test": "data"}' \
  http://localhost:3000/api/test
```

#### Environment Debugging
```bash
# Container debugging
docker exec -it container-name /bin/bash
kubectl describe pod pod-name
kubectl logs pod-name --previous

# Service debugging
systemctl status service-name
journalctl -u service-name --since "1 hour ago"
```

## Phase 4: Solution Implementation & Validation

### Fix Implementation Strategy

1. **Immediate Fixes (Hot Fixes)**:
   - Quick patches for critical issues
   - Rollback procedures ready
   - Minimal risk changes

2. **Systematic Fixes**:
   - Root cause resolution
   - Comprehensive testing
   - Documentation updates

3. **Preventive Measures**:
   - Monitoring and alerting improvements
   - Code quality enhancements
   - Process improvements

### Solution Validation Process
```bash
# Pre-deployment validation
npm run test
npm run lint
npm run build

# Deployment validation
kubectl apply --dry-run=client -f deployment.yaml
terraform plan

# Post-deployment validation
curl -f http://localhost:3000/health
npm run test:integration
```

## Phase 5: Knowledge Management & Learning

### Documentation Requirements

1. **Incident Report Template**:
   ```markdown
   # Incident Report: [Brief Description]
   
   **Date**: [Date]
   **Severity**: [Critical/High/Medium/Low]
   **Duration**: [How long the issue lasted]
   **Impact**: [What was affected]
   
   ## Timeline
   - [Time]: Issue detected
   - [Time]: Investigation started
   - [Time]: Root cause identified
   - [Time]: Fix implemented
   - [Time]: Issue resolved
   
   ## Root Cause
   [Detailed explanation of what caused the issue]
   
   ## Resolution
   [What was done to fix the issue]
   
   ## Prevention
   [What will be done to prevent similar issues]
   
   ## Lessons Learned
   [Key takeaways for the team]
   ```

2. **Solution Knowledge Base**:
   - Common issues and solutions
   - Debugging checklists
   - Environment-specific guides
   - Escalation procedures

### Learning Objectives

For each troubleshooting session, ensure:
- **Skill Development**: Teach debugging methodology
- **Knowledge Sharing**: Document solutions for team
- **Process Improvement**: Identify workflow enhancements
- **Prevention**: Implement measures to avoid recurrence

## Phase 6: Monitoring & Follow-up

### Post-Resolution Monitoring
```bash
# Set up monitoring
# Application metrics
curl -s http://localhost:3000/metrics | grep error_rate

# System metrics
watch -n 5 'ps aux | grep app'
watch -n 5 'netstat -an | grep :3000'

# Log monitoring
tail -f /var/log/app.log | grep -i error
```

### Escalation Procedures

1. **Level 1 - Self-Service**:
   - Use this troubleshooting guide
   - Check documentation and knowledge base
   - Attempt standard solutions

2. **Level 2 - Team Escalation**:
   - Involve senior team members
   - Use team communication channels
   - Share debugging information

3. **Level 3 - Expert Escalation**:
   - Engage domain experts
   - Vendor support if necessary
   - External consultants if required

### Communication Templates

#### Status Update Template
```
üîß TROUBLESHOOTING UPDATE

Issue: [Brief description]
Status: [Investigating/In Progress/Resolved]
ETA: [Expected resolution time]
Impact: [Current impact level]
Next Steps: [What's happening next]

Affected Systems: [List]
Workaround: [If available]
```

#### Resolution Communication
```
‚úÖ ISSUE RESOLVED

Issue: [Brief description]
Duration: [How long it took to resolve]
Root Cause: [What caused the issue]
Solution: [What fixed it]
Prevention: [What we're doing to prevent recurrence]

Thank you for your patience.
```

## Integration Points

### Logging Systems
- **Centralized Logging**: ELK Stack, Splunk, Fluentd
- **Application Logs**: Structured logging with correlation IDs
- **System Logs**: Syslog, journald, Docker logs

### Monitoring Systems
- **APM Tools**: New Relic, Datadog, AppDynamics
- **Infrastructure**: Prometheus, Grafana, Nagios
- **Synthetic Monitoring**: Pingdom, Uptime Robot

### Development Tools
- **IDEs**: VS Code debugging, IntelliJ debugger
- **Profilers**: Chrome DevTools, Java Flight Recorder
- **Testing**: Jest, PyTest, JUnit debugging modes

## Output Requirements

Provide a comprehensive troubleshooting report that includes:

1. **Executive Summary**:
   - Issue description and impact
   - Resolution status and timeline
   - Key findings and recommendations

2. **Technical Details**:
   - Root cause analysis
   - Debugging steps taken
   - Evidence and logs
   - Solution implementation

3. **Action Items**:
   - Immediate fixes required
   - Long-term improvements
   - Monitoring enhancements
   - Team training needs

4. **Knowledge Assets**:
   - Updated troubleshooting guides
   - Solution documentation
   - Process improvements
   - Lessons learned

5. **Follow-up Plan**:
   - Monitoring requirements
   - Review schedule
   - Success metrics
   - Escalation triggers

Remember: The goal is not just to fix the immediate problem, but to build debugging capabilities, improve processes, and prevent future issues. Every troubleshooting session is an opportunity to strengthen the team's problem-solving skills and the system's resilience.
</instructions>

<examples>
## Example 1: Production Error Troubleshooting

**Command**: `/troubleshoot error production critical`

**Scenario**: API endpoints returning 500 errors

**Response**:
```
üö® CRITICAL PRODUCTION ERROR - IMMEDIATE ACTION REQUIRED

TRIAGE ASSESSMENT:
- Issue Type: Production API errors
- Urgency: CRITICAL
- Context: Production environment
- Estimated Impact: High - affecting all users

IMMEDIATE ACTIONS TAKEN:
1. ‚úÖ Checked system health
2. ‚úÖ Verified recent deployments
3. ‚úÖ Examined error logs
4. ‚úÖ Assessed user impact

ROOT CAUSE IDENTIFIED:
Database connection pool exhaustion due to long-running queries from recent feature deployment.

RESOLUTION IMPLEMENTED:
1. Increased connection pool size (immediate)
2. Added query timeouts (immediate)
3. Rolled back problematic queries (immediate)
4. Scheduled query optimization (follow-up)

MONITORING ACTIVATED:
- Database connection metrics
- API response time alerts
- Error rate monitoring

STATUS: RESOLVED ‚úÖ
Recovery Time: 23 minutes
Next Review: 2 hours
```

## Example 2: Performance Investigation

**Command**: `/troubleshoot performance development medium`

**Scenario**: Slow page load times in development

**Response**:
```
üîç PERFORMANCE INVESTIGATION INITIATED

SYSTEMATIC ANALYSIS APPROACH:
1. Baseline measurement
2. Component profiling
3. Bottleneck identification
4. Optimization implementation

FINDINGS:
- Database queries: 2.3s average (target: <100ms)
- Bundle size: 2.1MB (target: <1MB)
- Image loading: 800ms (target: <200ms)

OPTIMIZATION PLAN:
1. Database query optimization
2. Code splitting implementation
3. Image optimization and lazy loading
4. Caching strategy implementation

LEARNING OBJECTIVES:
- Performance profiling techniques
- Database query optimization
- Frontend optimization strategies
- Monitoring and alerting setup
```

## Example 3: Environment Issues

**Command**: `/troubleshoot environment development low`

**Scenario**: Docker container failing to start

**Response**:
```
üîß ENVIRONMENT TROUBLESHOOTING

SYSTEMATIC DEBUGGING:
1. Container logs analysis
2. Dockerfile review
3. Dependency verification
4. Network configuration check

DEBUGGING WORKFLOW:
1. ‚úÖ Examined container logs
2. ‚úÖ Verified base image
3. ‚úÖ Checked port conflicts
4. ‚úÖ Validated environment variables

SOLUTION IDENTIFIED:
Port 3000 already in use by another service.

RESOLUTION:
1. Updated docker-compose.yml to use port 3001
2. Added port conflict detection script
3. Updated development documentation

KNOWLEDGE GAINED:
- Docker troubleshooting techniques
- Port management best practices
- Environment setup automation
```
</examples>

<thinking>
This troubleshooting command is designed to be comprehensive and educational, providing systematic debugging guidance while building team capabilities. The structure follows a logical progression from immediate triage through resolution and knowledge capture.

Key strengths of this implementation:
1. Urgency-based response protocols
2. Systematic debugging methodologies
3. AI-powered error analysis patterns
4. Comprehensive solution validation
5. Strong focus on learning and knowledge sharing
6. Integration with existing tools and systems
7. Clear communication templates
8. Follow-up and monitoring procedures

The command balances immediate problem-solving with long-term capability building, making it valuable for both crisis response and continuous improvement.
</thinking>
</xml>
```

I've created a comprehensive troubleshooting command file at `/home/sistrlls/Projects/ccprompts/.claude/commands/troubleshoot.md`. This command provides intelligent debugging assistance with the following key features:

## Core Capabilities

**Systematic Approach**: The command follows a structured 6-phase methodology:
1. **Problem Assessment & Triage** - Immediate information gathering and urgency-based response
2. **Intelligent Error Analysis** - AI-powered pattern recognition and symptom analysis
3. **Systematic Debugging Workflow** - Step-by-step debugging process with proven techniques
4. **Solution Implementation & Validation** - Comprehensive fix implementation and testing
5. **Knowledge Management & Learning** - Documentation and team learning emphasis
6. **Monitoring & Follow-up** - Post-resolution monitoring and escalation procedures

## Key Features

**Urgency-Based Response**: Different protocols for critical, high, medium, and low urgency issues
**Error Pattern Recognition**: Built-in analysis for common error categories (syntax, runtime, network, database, environment)
**Comprehensive Debugging Techniques**: Specific approaches for different issue types (error, performance, behavior, environment)
**Integration Points**: Connections to logging systems, monitoring tools, and development environments
**Learning Focus**: Every troubleshooting session includes skill development and knowledge sharing objectives

## Usage Examples

```bash
/troubleshoot error production critical     # Production emergency
/troubleshoot performance development medium # Performance optimization
/troubleshoot behavior staging high         # Unexpected behavior investigation
/troubleshoot environment development low   # Environment setup issues
```

## Practical Benefits

- **Immediate Action**: Provides quick triage and immediate response protocols
- **Educational Value**: Teaches debugging methodology while solving problems
- **Knowledge Building**: Creates documentation and builds team expertise
- **Integration Ready**: Works with existing logging, monitoring, and development tools
- **Communication**: Includes templates for status updates and resolution communications

The command emphasizes both problem-solving and capability building, making it valuable for crisis response and continuous improvement. It's designed to help developers learn debugging skills while providing systematic guidance for resolving immediate issues.